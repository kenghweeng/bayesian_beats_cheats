{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This implements the iterative classification algorithm as described in slide 37 of http://web.stanford.edu/class/cs224w/slides/05-message.pdf  \n",
    "It classifies a node based on its features as well as labels of neighbours\n",
    "\n",
    "## Definitions\n",
    "$v$: Node  \n",
    "$Y_v$: Labels of node $v$  \n",
    "$f_v$: feature vector of node $v$  \n",
    "$z_v$: summary of labels of $v$'s neighbours (a vector)  \n",
    "$\\phi_1(f_v)$: predict node label based on node feature vector $f_v$  \n",
    "$\\phi_2(f_v, z_v)$: predict label based on node feature vector $f_v$ of labels of $v$'s neighbours\n",
    "\n",
    "## Phase 1: Train a Classifier based on node attributes only\n",
    "The classifier can be linear classifier, neural network classifier etc. This is trained on the training set to predict the labels for each node.\n",
    "\n",
    "$\\phi_1(f_v)$ : to predict $Y_v$ based on $f_v$  \n",
    "$\\phi_2(f_v, z_v)$ to predict $Y_v$ based on $f_v$ and summary $z_v$ of labels of $v$'s neighbours  \n",
    "For vector $z_v$ of neighbourhood labels, let\n",
    "\n",
    "- $I$ = incoming neighbour label info vector  \n",
    "  $I_0$ = 1 if at least one of the incoming node is labelled 0.  \n",
    "  $I_1$ = 1 if at least one of the incoming node is labelled 1.\n",
    "- $O$ = outgoing neighbour label info vector  \n",
    "  $O_0$ = 1 if at least one of the outgoing node is labelled 1.  \n",
    "  $O_1$ = 1 if at least one of the outgoing node is labelled 1.\n",
    "\n",
    "## Phase 2: Iterate till Convergence\n",
    "\n",
    "On the test set, set the labels based on the classifier in Phase 1,\n",
    "\n",
    "## Step 1: Train Classifier\n",
    "\n",
    "On a different training set, train two classifiers:\n",
    "\n",
    "- node attribute vector only: $\\phi_1$\n",
    "- node attribute and link vectors: $\\phi_2$\n",
    "\n",
    "## Step 2: Apply Classifier to test set\n",
    "\n",
    "On test set, use trained node feature vector classifier $\\phi_1$ to set $Y_v$\n",
    "\n",
    "## Step 3.1: Update relational vectors z\n",
    "\n",
    "Update $z_v$ for all nodes on test set\n",
    "\n",
    "## 3.2: Update Label\n",
    "\n",
    "Reclassify all nodes with $\\phi_2$\n",
    "\n",
    "## Iterate\n",
    "\n",
    "Continue until convergence\n",
    "\n",
    "- update $z_v$\n",
    "- update $Y_v = \\phi_2(f_v, z_v)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collective.constants import get_summary_zv\n",
    "from collective.Iterative import IterativeClassification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "import parse\n",
    "import sys\n",
    "sys.path.insert(1, '../src')\n",
    "import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_node = pd.read_csv('../data/unified_node_data.csv', keep_default_na=False)\n",
    "df_edge = pd.read_csv('../data/max_edge_weights.csv')\n",
    "df_formatted = preprocess.nodes1(df_node)\n",
    "df_clean = preprocess.nodes_filter(df_formatted, df_edge)\n",
    "df_impute = preprocess.impute(df_clean)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = preprocess.stratified_train_val_test_split(df_impute)\n",
    "X_test = pd.concat([X_val, X_test])\n",
    "y_test = pd.concat([y_val, y_test])\n",
    "\n",
    "y_train = y_train.apply(lambda x: 1 if x > 0 else 0).rename(\"label\")\n",
    "y_test = y_test.apply(lambda x: 1 if x > 0 else 0).rename(\"label\")\n",
    "\n",
    "X_train = pd.concat([X_train, y_train], axis = 1)\n",
    "X_train = X_train.drop(['confessed_assignments'], axis = 1)\n",
    "X_test = pd.concat([X_test, y_test], axis = 1)\n",
    "X_test = X_test.drop(['confessed_assignments'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model2\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[264,   0],\n",
       "       [ 44,   0]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training model2\")\n",
    "train_x_model2 = X_train.drop(['label', 'name'], axis=1)\n",
    "train_y_model2 = X_train['label']\n",
    "test_x_model2 = X_test.drop(['label', 'name'], axis=1)\n",
    "test_y_model2 = X_test['label']\n",
    "model2 = LogisticRegression(max_iter = 1000)\n",
    "model2.fit(train_x_model2, train_y_model2)\n",
    "y_pred2 = model2.predict(test_x_model2)\n",
    "\n",
    "print(f1_score(test_y_model2.to_numpy(), y_pred2))\n",
    "confusion_matrix(test_y_model2.to_numpy(), y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further cleaning\n",
    "Note that need to drop confessed_assignments and num_confessed_assignments as both indicates whether the student cheated or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_graph_train = parse.create_nx_graph_nodes(X_train)\n",
    "network_graph_train = parse.add_nx_graph_edges(network_graph_train, df_edge)\n",
    "\n",
    "network_graph_test = parse.create_nx_graph_nodes(X_test)\n",
    "network_graph_test = parse.add_nx_graph_edges(network_graph_test, df_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gets L1_max, L0_max, L1_mean, L0_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_graph_train = get_summary_zv(network_graph_train)\n",
    "network_graph_test = get_summary_zv(network_graph_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame()\n",
    "for node in network_graph_train.nodes:\n",
    "    network_graph_train.nodes[node]['index'] = node\n",
    "    temp = pd.DataFrame([network_graph_train.nodes[node]]).set_index('index')\n",
    "    df_train = pd.concat([df_train, temp])\n",
    "\n",
    "df_test = pd.DataFrame()\n",
    "for node in network_graph_test.nodes:\n",
    "    network_graph_test.nodes[node]['index'] = node\n",
    "    temp = pd.DataFrame([network_graph_test.nodes[node]]).set_index('index')\n",
    "    df_test = pd.concat([df_test, temp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Logistic Regression without L1_max, L0_max, L1_mean, L0_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1\n",
    "print(\"Training model1\")\n",
    "train_x_model1 = df_train.drop(['L1_max', 'L0_max', 'L1_mean', 'L0_mean', 'label'], axis=1)\n",
    "train_y_model1 = df_train['label']\n",
    "test_x_model1 = df_test.drop(['L1_max', 'L0_max', 'L1_mean', 'L0_mean', 'label'], axis=1)\n",
    "test_y_model1 = df_test['label']\n",
    "model1 = LogisticRegression(max_iter = 1000)\n",
    "model1.fit(train_x_model1, train_y_model1)\n",
    "y_pred1 = model1.predict(test_x_model1)\n",
    "\n",
    "print(f1_score(test_y_model1.to_numpy(), y_pred1))\n",
    "confusion_matrix(test_y_model1.to_numpy(), y_pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Logistic Regression with L1_max, L0_max, L1_mean, L0_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training model2\")\n",
    "train_x_model2 = df_train.drop(['label'], axis=1)\n",
    "train_y_model2 = df_train['label']\n",
    "test_x_model2 = df_test.drop(['label'], axis=1)\n",
    "test_y_model2 = df_test['label']\n",
    "model2 = LogisticRegression(max_iter = 1000)\n",
    "model2.fit(train_x_model2, train_y_model2)\n",
    "y_pred2 = model2.predict(test_x_model2)\n",
    "\n",
    "print(f1_score(test_y_model2.to_numpy(), y_pred2))\n",
    "confusion_matrix(test_y_model2.to_numpy(), y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iterative classification\")\n",
    "ic = IterativeClassification(max_iterations=5)\n",
    "new_gnx = ic.predict(network_graph_test, model1, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_gnx_pred = pd.DataFrame([])\n",
    "for node in df_test['label'].index:\n",
    "    temp = pd.DataFrame([[new_gnx.nodes[node]['label'][0], node]], columns=[\n",
    "                        'label', 'index']).set_index('index')\n",
    "    new_gnx_pred = pd.concat([new_gnx_pred, temp])\n",
    "print(f1_score(test_y_model2, new_gnx_pred.to_numpy()))\n",
    "confusion_matrix(test_y_model2, new_gnx_pred.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
